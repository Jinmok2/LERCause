{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NFQleLIQc6F"
   },
   "source": [
    "# LERCause: Causal Sentence Identification with LER (Nuclear Safety Reports)     \n",
    "\n",
    "This code is to run a CNN (Convolutional Neural Networks) model on LER data for sentence classification and prediction.    \n",
    "      \n",
    "Author:   \n",
    "1. Jinmo Kim: School of Information Sciences, University of Illinois Urbana-Champaign   \n",
    "2. Jenna Kim: School of Information Sciences, Univeristy of Illinois Urbana-Champaign      \n",
    "\n",
    "Cite this paper:   \n",
    "\n",
    "Kim, J., Kim, J., Lee, A., Kim, J., Diesner, J. (2024). LERCause: Deep learning approaches for causal sentence identification from nuclear safety reports. Plos One.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY3m9eQWQc6K"
   },
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJiVCXKOy1iw"
   },
   "source": [
    "## 1-1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxyXO-krQc6K",
    "outputId": "6721e1e2-3393-4ce5-9eee-fc7f84b62a1a"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import gc\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install imbalanced-learn library for sampling if not already installed\n",
    " \n",
    "#!pip install imbalanced-learn==0.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9UhXfGky7TE"
   },
   "source": [
    "## 1-2. Check GPU settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "661lHApECraH"
   },
   "source": [
    "Make sure to use \"conda_tensorflow2_p310\" kernel to run this code in AWS Sagemaker. If not setup, you can find it go to Kernel -> Change kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lWr7vKYQc6T",
    "outputId": "68fa7b5f-50b4-4aa1-f34a-2b619e137327"
   },
   "outputs": [],
   "source": [
    "# check the version of Tensorflow and Keras\n",
    "# Tensorflow (ver 2.12.0); Keras (ver 2.12.0)\n",
    "\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Keras version: \", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPQAI_5ryYU-",
    "outputId": "4a64ee78-e764-4a97-9ce4-e4df53a84940"
   },
   "outputs": [],
   "source": [
    "# check if gpu is available\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU device: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js_T9kAT1MY7"
   },
   "source": [
    "TensorFlow supports running computations on a variety of types of devices, including CPU and GPU. They are reperesented with string identifiers.    \n",
    "\n",
    "For example:  \n",
    "\"/device:CPU:0\" : CPU of your machine  \n",
    "\"/physical_device:GPU:0\": GPU visible to TensorFlow.  \n",
    "\n",
    "TensorFlow code, with Keras included, can run on a GPU by default without requiring explicit code configuration. If both CPU and GPU are available, TensorFlow will run the GPU-capable code unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_dyNsfuCraK",
    "outputId": "fe7c768e-896a-41ab-9ca7-fba72f51662f"
   },
   "outputs": [],
   "source": [
    "# check GPU memory and & utilization\n",
    "!nvidia-smi\n",
    "\n",
    "# If want to clear occupied memory\n",
    "#import gc\n",
    "#gc.collect()\n",
    "\n",
    "# check GPU memory usage again\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBuQ-mWHQc6L"
   },
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGk8h0qiC5io"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, record):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file   \n",
    "    record: text file to include a processing output\n",
    "    \n",
    "    return two dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Read in data from input file '''\n",
    "    df = pd.read_csv(filename, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    ''' Display no of rows and columns '''\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record)\n",
    "    \n",
    "    \n",
    "    ''' Select data needed for processing & rename columns '''\n",
    "    df = df[['PMID', 'USENID', 'SENT', 'CLASS']]\n",
    "    df.rename({\"PMID\": \"pmid\", \"USENID\": \"usenid\", \"SENT\": \"sentence\", \"CLASS\": \"label\"}, \n",
    "              axis=1, \n",
    "              inplace=True)\n",
    "   \n",
    "\n",
    "    ''' Remove null values & trim string data '''\n",
    "    df=df.dropna()\n",
    "    df[\"sentence\"] = df[\"sentence\"].apply(lambda x: x.strip())\n",
    "    \n",
    "    print(\"No of rows (After removing null): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "    print(\"No of rows (After removing null): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of columns: {}\".format(df.shape[1]), file=record) \n",
    "    \n",
    "    \n",
    "    ''' Check the first few instances '''  \n",
    "    print(\"\\n<Data View: First Few Instances>\")\n",
    "    print(\"\\n\", df.head()) \n",
    "    print(\"\\n<Data View: First Few Instances>\", file=record)\n",
    "    print(\"\\n\", df.head(), file=record)\n",
    "    \n",
    "    \n",
    "    ''' Display no of lables and rows ''' \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)\n",
    "    \n",
    "\n",
    "    ''' Split data into X and y '''\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "     \n",
    "     \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "488zJQ7Uii4j"
   },
   "outputs": [],
   "source": [
    "def sample_data(X_train, \n",
    "                y_train, \n",
    "                record, \n",
    "                sampling=0, \n",
    "                sample_method='over'):  \n",
    "    \"\"\"\n",
    "       Sampling input train data\n",
    "       \n",
    "       X_train: dataframe of X train data\n",
    "       y_train: datafram of y train data\n",
    "       record: text file including a processing output\n",
    "       sampling: indicator of sampling funtion is on or off\n",
    "       sample_method: method of sampling (oversampling or undersampling)\n",
    "       \n",
    "       return two sampled dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    \n",
    "    \n",
    "    ''' Select a sampling method '''\n",
    "    if sampling:\n",
    "        if sample_method == 'over':\n",
    "            oversample = RandomOverSampler(random_state=42)\n",
    "            X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "            print('\\n************** Data Sampling **************')\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()))\n",
    "            print('\\n************** Data Sampling **************', file=record)\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()), file=record)\n",
    "            \n",
    "            X_train_sam, y_train_sam = X_over, y_over\n",
    "            \n",
    "        elif sample_method == 'under':\n",
    "            undersample = RandomUnderSampler(random_state=42)\n",
    "            X_under, y_under = undersample.fit_resample(X_train, y_train)\n",
    "        \n",
    "            print('\\n************** Data Sampling **************')\n",
    "            print('\\nUndersampled Data (class, Rows):\\n{}'.format(y_under.value_counts()))\n",
    "            print('\\n************** Data Sampling **************', file=record)\n",
    "            print('\\nUndersampled Data (class, Rows):\\n{}'.format(y_under.value_counts()), file=record)\n",
    "            \n",
    "            X_train_sam, y_train_sam = X_under, y_under\n",
    "    else:\n",
    "        X_train_sam, y_train_sam = X_train, y_train \n",
    "        \n",
    "        print('\\n************** Data Sampling **************')\n",
    "        print('\\nNo Sampling Performed\\n')\n",
    "        print('\\n************** Data Sampling **************', file=record)\n",
    "        print('\\nNo Sampling Performed\\n', file=record)\n",
    "    \n",
    "    return X_train_sam, y_train_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMhEpPDaGM_X"
   },
   "outputs": [],
   "source": [
    "def token_distribution(df):\n",
    "    \"\"\"\n",
    "       Display a distribution of tokens\n",
    "       \n",
    "       df: a dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    token_lens = []\n",
    "    long_tokens = []\n",
    "    \n",
    "    for text in df.sentence.to_list():\n",
    "        \n",
    "        ''' Split text into tokens '''\n",
    "        tokens = text.split()\n",
    "        token_lens.append(len(tokens))\n",
    "    \n",
    "        ''' Check a sentence with extreme length '''\n",
    "        if len(tokens) > 150:\n",
    "            long_tokens.append(len(tokens))   \n",
    "\n",
    "    print(\"\\n************* Token Distribution: train data *************\")\n",
    "    print(\"long sentences: \")\n",
    "    print(long_tokens)\n",
    "    \n",
    "    \n",
    "    ''' Plot the distribution '''\n",
    "    print(\"Min token:\", min(token_lens))\n",
    "    print(\"Max token:\", max(token_lens))\n",
    "    print(\"Avg token:\", round(np.mean(token_lens)))\n",
    "\n",
    "    sns.displot(token_lens)\n",
    "    plt.xlim([0,max(token_lens)+10])\n",
    "    plt.xlabel(\"Token Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFnFxlLdQc6V"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    \"\"\"\n",
    "       Plot loss and accuracy of training & validation\n",
    "       \n",
    "       history: a dictionary containing a summary of training and valiadation scores\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    ''' Extract scores '''\n",
    "    acc = history.history['binary_accuracy']\n",
    "    val_acc = history.history['val_binary_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    \n",
    "    ''' Create a plot '''\n",
    "    x = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsEIfStWuyEh"
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(maxlen, \n",
    "                     vocab_size, \n",
    "                     record):\n",
    "    \"\"\"\n",
    "       Instantiate a cnn model\n",
    "       \n",
    "       max_len: maximum input length\n",
    "       vocab_size: size of vocabulary\n",
    "       record: text file including a processing output\n",
    "       \n",
    "       return a configured model\n",
    "    \"\"\"\n",
    "    embedding_dim = 100\n",
    "  \n",
    "    ''' Define the model '''\n",
    "    model = Sequential()\n",
    "\n",
    "    \n",
    "    ''' Add an embedding layer '''\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                               output_dim=embedding_dim, \n",
    "                               input_length=maxlen))\n",
    "\n",
    "    \n",
    "    ''' Add a first convolutional layer with 512 filters '''\n",
    "    model.add(layers.Conv1D(512, 2, activation='relu'))\n",
    "  \n",
    "\n",
    "    ''' Pooling layer '''\n",
    "    #model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.MaxPooling1D())\n",
    "\n",
    "    \n",
    "    ''' Add a second convolutional layer '''\n",
    "    model.add(layers.Conv1D(512, 3, activation='relu'))\n",
    "\n",
    "    \n",
    "    ''' Add a second pooling layer '''\n",
    "    model.add(layers.MaxPooling1D())\n",
    "  \n",
    "    \n",
    "    ''' Flattening '''\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    \n",
    "    ''' Add dropout to prevent overfitting '''\n",
    "    model.add(layers.Dropout(0.5))\n",
    "  \n",
    "\n",
    "    ''' Full connection '''\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "  \n",
    "\n",
    "    ''' Compile the model '''\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['binary_accuracy',\n",
    "                           tf.keras.metrics.Precision(name='precision'),\n",
    "                           tf.keras.metrics.Recall(name='recall')])\n",
    "  \n",
    "    ''' Summarize the model '''\n",
    "    print(\"\\n\\n************* Model Summary *************\\n\")\n",
    "    print(model.summary(), \"\\n\")\n",
    "    print(\"\\n\\n************* Model Summary *************\\n\", file=record)\n",
    "    model.summary(print_fn=lambda x: record.write(x + '\\n'))\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, \n",
    "                   y_pred, \n",
    "                   record, \n",
    "                   eval_model=0):\n",
    "    \"\"\"\n",
    "      evaluate a model performance\n",
    "      \n",
    "      y_test: original y test data\n",
    "      y_pred: predicted y values\n",
    "      record: text file containing a processing output\n",
    "      eval_model: indicator if this funtion is on or off\n",
    "    \"\"\"\n",
    "    \n",
    "    if eval_model:\n",
    "        \n",
    "        ''' Create a confusion matrix '''\n",
    "        print('\\nConfusion Matrix:\\n')\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print('\\nConfusion Matrix:\\n', file=record)\n",
    "        print(confusion_matrix(y_test, y_pred), file=record)\n",
    "        \n",
    "        ''' Display a classification report '''\n",
    "        print('\\nClassification Report:\\n')\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "        print('\\nClassification Report:\\n', file=record)\n",
    "        print(classification_report(y_test, y_pred, digits=4), file=record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(predictions, \n",
    "                  X_test, \n",
    "                  y_test, \n",
    "                  y_pred, \n",
    "                  proba_file, \n",
    "                  proba_on=0):\n",
    "    \n",
    "    \"\"\"\n",
    "       Predict probability of each class\n",
    "       \n",
    "       predictions: a list of probability scores for class 1\n",
    "       X_test: original X test data\n",
    "       y_test: original y test data\n",
    "       y_pred: predicted y values\n",
    "       proba_file: output file of probability scores\n",
    "       proba_on: decide if the probability output is expected    \n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    if proba_on:\n",
    "        \n",
    "        ''' Compute probability '''\n",
    "        y_prob = predictions\n",
    "        df_prob = pd.DataFrame(data=y_prob, columns=[\"prob_1\"])\n",
    "        result = pd.concat([X_test.reset_index(drop=True), df_prob], \n",
    "                           axis=1, \n",
    "                           ignore_index=False)\n",
    "    \n",
    "    \n",
    "        ''' Add predicted class to output '''\n",
    "        result['pred'] = pd.Series(y_pred)\n",
    "\n",
    "\n",
    "        ''' Add actual class to output ''' \n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        result['act'] = y_test\n",
    "\n",
    "        \n",
    "        ''' Save output '''\n",
    "        result.to_csv(proba_file, \n",
    "                      encoding='utf-8', \n",
    "                      index=False, \n",
    "                      header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(input_file, result_file):\n",
    "    \n",
    "    \"\"\"\n",
    "       Split data from input file\n",
    "       \n",
    "       input_file: file containing input data \n",
    "       result_file: name of output file of evaluation\n",
    "       \n",
    "       return X and y dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Open result file for records '''\n",
    "    f=open(result_file, \"a\")\n",
    "    \n",
    "    \n",
    "    ''' Load data '''\n",
    "    print(\"\\n************** Loading Data ************\\n\")\n",
    "    print(\"\\n************** Loading Data ************\\n\", file=f)\n",
    "    \n",
    "    X, y = load_data(input_file, record=f)\n",
    "    \n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]))\n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]), file=f)\n",
    "    \n",
    "    \n",
    "    ''' Train and test split '''\n",
    "    print(\"\\n************** Spliting Data **************\\n\")\n",
    "    print(\"\\n************** Spliting Data **************\\n\", file=f)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(X_train.shape))\n",
    "    print(\"Val Data: {}\".format(X_val.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "    print(\"Train Data: {}\".format(X_train.shape), file=f)\n",
    "    print(\"Val Data: {}\".format(X_val.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(X_test.shape), file=f)\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train')\n",
    "    print(y_train.value_counts())\n",
    "    print('\\nClass Counts(label, row): Val')\n",
    "    print(y_val.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test')\n",
    "    print(y_test.value_counts())    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(y_train.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Val', file=f)\n",
    "    print(y_val.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(y_test.value_counts(), file=f)\n",
    "\n",
    "    print(\"\\n<X_train Data>\")\n",
    "    print(X_train.head())\n",
    "    print(\"\\n<X_train Data>\", file=f)\n",
    "    print(X_train.head(), file=f)\n",
    "    \n",
    "    print(\"\\n<X_val Data>\")\n",
    "    print(X_val.head())\n",
    "    print(\"\\n<X_val Data>\", file=f)\n",
    "    print(X_val.head(), file=f)\n",
    "    \n",
    "    print(\"\\n<X_test Data>\")\n",
    "    print(X_test.head())\n",
    "    print(\"\\n<X_test Data>\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "    \n",
    "\n",
    "    return (X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X_train, \n",
    "                y_train,\n",
    "                X_val, \n",
    "                y_val,\n",
    "                datasize_change,\n",
    "                sample_balance,\n",
    "                balance_sampling_on,                                   \n",
    "                balance_sampling_type,\n",
    "                sample_ratio,\n",
    "                ratio,\n",
    "                sample_on, \n",
    "                sample_type, \n",
    "                tokernizer_file,\n",
    "                max_len,\n",
    "                batch_size,\n",
    "                epochs,\n",
    "                model_file,\n",
    "                result_file):   \n",
    "    \"\"\"\n",
    "       Function for data processing and model fitting\n",
    "       \n",
    "       X_train: dataframe containing X train data \n",
    "       y_train: dataframe containing y train data\n",
    "       X_val: dataframe containing X validation data \n",
    "       y_val: dataframe containing y validation data\n",
    "       datasize_change: data size change on or off\n",
    "       sample_balance: balance of sample on or off\n",
    "       balance_sampling_on: sampling on or off when balance is 1\n",
    "       balance_samplling_type: sample type to choose if balance_sampling_on is 1\n",
    "       sample_ratio: proportion of data size for balance sampling\n",
    "       ratio: proportion of data size\n",
    "       sample_on: sampling on or off\n",
    "       sample_type: sample type to choose if sample_on is 1\n",
    "       tokenizer_file: file to save tokenizer\n",
    "       max_len: maximun length of tokens\n",
    "       batch_size: size of batch \n",
    "       epochs: number of epoch\n",
    "       model_file: file saved trained model\n",
    "       result_file: name of output file of evaluation  \n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Open result file for records '''\n",
    "    f = result_file\n",
    "    \n",
    "    \n",
    "    ''' Data size change '''\n",
    "    if datasize_change:\n",
    "        \n",
    "        ''' Sample data with balance (1:1) '''\n",
    "        if sample_balance:\n",
    "            \n",
    "            print(\"\\n************** Data Balancing: Label Class (1:1) *************\\n\")\n",
    "            print(\"\\n************** Data Balancing: Label Class (1:1) *************\\n\", file=f)\n",
    "            \n",
    "            X_train, y_train = sample_data(X_train, \n",
    "                                           y_train, \n",
    "                                           record=f, \n",
    "                                           sampling=balance_sampling_on, \n",
    "                                           sample_method=balance_sampling_type)\n",
    "                      \n",
    "            print('\\nClass Counts(label, row): After balancing')\n",
    "            print(y_train.value_counts())\n",
    "            print('\\nClass Counts(label, row): After balancing', file=f)\n",
    "            print(y_train.value_counts(), file=f)\n",
    "            \n",
    "            print(\"\\n<Balanced Train Data>\")\n",
    "            print(X_train.head()) \n",
    "            print(\"\\n<Balanced Train Data>\", file=f)\n",
    "            print(X_train.head(), file=f)\n",
    "           \n",
    "        \n",
    "        ''' Sample data based on size ratio '''   \n",
    "        if sample_ratio:\n",
    "            if ratio == 1:\n",
    "                X_train = X_train\n",
    "                y_train = y_train       \n",
    "            else:\n",
    "                X_train, _, y_train, _ = train_test_split(X_train, \n",
    "                                                          y_train, \n",
    "                                                          train_size=ratio, \n",
    "                                                          random_state=42, \n",
    "                                                          stratify=y_train)\n",
    "                \n",
    "            print(\"\\n************** Data Size Change: Ratio *************\\n\")\n",
    "            print(\"Data Ratio: {}\".format(ratio))   \n",
    "            print(\"\\n************** Data Size Change: Ratio *************\\n\", file=f)\n",
    "            print(\"Data Ratio: {}\".format(ratio), file=f)\n",
    "     \n",
    "            print('\\nClass Counts(label, row): After sampling')\n",
    "            print(y_train.value_counts())\n",
    "            print('\\nClass Counts(label, row): After sampling', file=f)\n",
    "            print(y_train.value_counts(), file=f)\n",
    "            \n",
    "            print(\"\\n<Train Data Based on Ratio>\")\n",
    "            print(X_train.head())\n",
    "            print(\"\\n<Train Data Based on Ratio>\", file=f)\n",
    "            print(X_train.head(), file=f)\n",
    "        \n",
    "    \n",
    "        ''' Reset index '''\n",
    "        X_train=X_train.reset_index(drop=True)\n",
    "        y_train=y_train.reset_index(drop=True)\n",
    "        X_val=X_val.reset_index(drop=True)\n",
    "        y_val=y_val.reset_index(drop=True)\n",
    "        \n",
    "        print(\"\\n************** Processing Data **************\")\n",
    "        print(\"\\nTrain Data: {}\".format(X_train.shape))\n",
    "        print(\"Val Data: {}\".format(X_val.shape))\n",
    "        print(\"\\n************** Processing Data **************\", file=f)\n",
    "        print(\"\\nTrain Data: {}\".format(X_train.shape), file=f)\n",
    "        print(\"Val Data: {}\".format(X_val.shape), file=f)\n",
    "\n",
    "        print('\\nClass Counts(label, row): Train')\n",
    "        print(y_train.value_counts())\n",
    "        print('\\nClass Counts(label, row): Train', file=f)\n",
    "        print(y_train.value_counts(), file=f)\n",
    "        print('\\nClass Counts(label, row): Val')\n",
    "        print(y_val.value_counts())\n",
    "        print('\\nClass Counts(label, row): Val', file=f)\n",
    "        print(y_val.value_counts(), file=f)\n",
    "        \n",
    "        print(\"\\n<X_train Data>\\n\")\n",
    "        print(X_train.head())\n",
    "        print(\"\\n<X_val Data>\")\n",
    "        print(X_val.head())\n",
    "        print(\"\\n<X_train Data>\\n\", file=f)\n",
    "        print(X_train.head(), file=f)\n",
    "        print(\"\\n<X_val Data>\", file=f)\n",
    "        print(X_val.head(), file=f)\n",
    "        \n",
    "    \n",
    "    ''' Sampling '''  \n",
    "    if sample_on:\n",
    "        X_train, y_train = sample_data(X_train, \n",
    "                                       y_train, \n",
    "                                       record=f, \n",
    "                                       sampling=sample_on, \n",
    "                                       sample_method=sample_type)\n",
    "        \n",
    "        print(\"\\nSampled Data: First Few Instances\")\n",
    "        print(X_train.head(3))\n",
    "        print(\"\\nSampled Data: First Few Instances\", file=f)\n",
    "        print(X_train.head(3), file=f)\n",
    "        \n",
    "\n",
    "    ''' Transform data '''\n",
    "    print(\"\\n************** Transforming Text into Vectors **************\")\n",
    "    print(\"\\n************** Transforming Text into Vectors **************\", file=f)\n",
    "    \n",
    "    sentences_train = X_train.iloc[:, -1]\n",
    "    sentences_val = X_val.iloc[:, -1]\n",
    "\n",
    "    print(\"\\nsentences_train: \", sentences_train.shape)\n",
    "    print(sentences_train.head())\n",
    "    print(\"\\nsentences_val: \", sentences_val.shape)\n",
    "    print(sentences_val.head())\n",
    "    print(\"\\nsentences_train: \", sentences_train.shape, file=f)\n",
    "    print(sentences_train.head(), file=f)\n",
    "    print(\"\\nsentences_val: \", sentences_val.shape, file=f)\n",
    "    print(sentences_val.head(), file=f)\n",
    "    \n",
    "    \n",
    "    ''' Tokenization '''\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    print(\"\\nvocab size: \", vocab_size)\n",
    "    print(\"\\nvocab size: \", vocab_size, file=f)\n",
    "    \n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    X_val = tokenizer.texts_to_sequences(sentences_val)\n",
    "    \n",
    "    print(\"\\nFirst Instance: Train\\n\")\n",
    "    print(sentences_train.iloc[0])\n",
    "    print(\"\\nFirst Instance: Train\\n\", file=f)\n",
    "    print(sentences_train.iloc[0], file=f)\n",
    "    \n",
    "    print(\"\\nFirst Instance: Val\\n\")\n",
    "    print(sentences_val.iloc[0])\n",
    "    print(\"\\nFirst Instance: Val\\n\", file=f)\n",
    "    print(sentences_val.iloc[0], file=f)\n",
    "    \n",
    "    \n",
    "    ''' Save the tokenizer '''\n",
    "    with open(tokenizer_file, 'wb') as tf:\n",
    "        pickle.dump(tokenizer, tf, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print(\"\\nTokenizer file '\" + tokenizer_file + \"' saved in the local directory\\n\", file=f)\n",
    "    print(\"\\nTokenizer file '\" + tokenizer_file + \"' saved in the local directory\\n\")\n",
    "    \n",
    "    \n",
    "    ''' Pad texts to a pre-defined max length '''\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "    X_val = pad_sequences(X_val, padding='post', maxlen=max_len)\n",
    "    \n",
    "    print(\"\\n<X_train vector: first instance>\\n\")\n",
    "    print(X_train[0, :])\n",
    "    print(\"\\n<X_train vector: first instance>\\n\", file=f)\n",
    "    print(X_train[0, :], file=f)\n",
    "\n",
    "    \n",
    "    ''' Model Fitting '''\n",
    "    print(\"\\n************** Training Model: CNN **************\")\n",
    "    print(\"\\n************** Training Model: CNN **************\", file=f)\n",
    "\n",
    "    model = create_cnn_model(max_len, \n",
    "                             vocab_size, \n",
    "                             record=f)\n",
    "    \n",
    "    \n",
    "    ''' Create a checkpoint to save the best model while training '''\n",
    "    model_checkpoint = ModelCheckpoint(model_file, \n",
    "                                       verbose=1, \n",
    "                                       monitor=\"val_binary_accuracy\",   \n",
    "                                       save_best_only=True, \n",
    "                                       save_weights_only=False,\n",
    "                                       mode=\"auto\", \n",
    "                                       save_freq=\"epoch\")  \n",
    "    \n",
    "    \n",
    "    ''' Train model with train and validation data '''\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[model_checkpoint])\n",
    "    \n",
    "    print(\"\\nTrained model '\" + model_file + \"' saved in the local directory\\n\")\n",
    "    print(\"\\nTrained model '\" + model_file + \"' saved in the local directory\\n\", file=f)\n",
    "    \n",
    "    \n",
    "    ''' Display a plot of loss & accuracy '''\n",
    "    print(\"\\n\")\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(X_test,\n",
    "                    y_test,\n",
    "                    tokenizer_file,\n",
    "                    model_file,\n",
    "                    max_len,\n",
    "                    eval_on, \n",
    "                    proba_file,\n",
    "                    proba_on,\n",
    "                    result_file):   \n",
    "    \"\"\"\n",
    "       Function for prediction and evaluation\n",
    "       \n",
    "       X_test: dataframe containing X test data \n",
    "       y_test: dataframe containing y test data\n",
    "       tokenizer_file: file containing saved tokenizer\n",
    "       model_file: file containing trained model\n",
    "       max_len: maximun length of tokens\n",
    "       eval_on: model evaluation on or off\n",
    "       proba_file: name of output file of probability\n",
    "       proba_on: probability on or off\n",
    "       result_file: name of output file of evaluation\n",
    "       \n",
    "    \"\"\"\n",
    "      \n",
    "    ''' Open result file for records '''\n",
    "    f = result_file\n",
    "    \n",
    "    ''' Load tokenizer and transform test data '''\n",
    "    print(\"\\nTest Data: First Few Instances\")\n",
    "    print(X_test.head())\n",
    "    print(\"\\nTest Data: First Few Instances\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "        \n",
    "    with open(tokenizer_file, 'rb') as tf:\n",
    "        tokenizer = pickle.load(tf)\n",
    "    \n",
    "    print(\"\\nA tokenizer from '\" + tokenizer_file + \"' loaded\")\n",
    "    print(\"\\nA tokenizer from '\" + tokenizer_file + \"' loaded\", file=f)\n",
    "    \n",
    "    sentences_test = X_test.iloc[:, -1]\n",
    "    X_test_trans = tokenizer.texts_to_sequences(sentences_test)\n",
    "    \n",
    "    print(\"\\nFirst Instance: Test\\n\")\n",
    "    print(sentences_test.iloc[0])\n",
    "    print(\"\\nFirst Instance: Test\\n\", file=f)\n",
    "    print(sentences_test.iloc[0], file=f)\n",
    "    \n",
    "    \n",
    "    ''' Pad texts to a pre-defined max length '''\n",
    "    X_test_trans = pad_sequences(X_test_trans, \n",
    "                                 padding='post', \n",
    "                                 maxlen=max_len)\n",
    "    \n",
    "    print(\"\\n<X_test vector: first instance>\\n\")\n",
    "    print(X_test_trans[0, :])\n",
    "    print(\"\\n<X_test vector: first instance>\\n\", file=f)\n",
    "    print(X_test_trans[0, :], file=f)\n",
    "    \n",
    "    \n",
    "    ''' Load trained model with its weights and optimizer '''\n",
    "    cnn_model = load_model(model_file)\n",
    "    \n",
    "    print(\"\\nA trained model from '\" + model_file + \"' loaded\")\n",
    "    print(\"\\nA trained model from '\" + model_file + \"' loaded\", file=f)\n",
    "    \n",
    "    \n",
    "    ''' Show the architecture of loaded model '''\n",
    "    print(\"\\n\\n************* Summary of Loaded Model *************\\n\")\n",
    "    print(cnn_model.summary())\n",
    "    print(\"\\n\\n************* Summary of Loaded Model *************\\n\", file=f)\n",
    "    cnn_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    \n",
    "    ''' Prediction ''' \n",
    "    print(\"\\n************** Getting Predictions **************\")\n",
    "    print(\"\\n************** Getting Predictions **************\", file=f)\n",
    "\n",
    "    predictions = cnn_model.predict(X_test_trans)\n",
    "    y_pred = (predictions > 0.5).astype(\"int32\").flatten()  \n",
    "\n",
    "    \n",
    "    ''' Evaluating model performance '''\n",
    "    print(\"\\n************** Evaluating Performance **************\")\n",
    "    print(\"\\n************** Evaluating Performance **************\", file=f)\n",
    "    \n",
    "    evaluate_model(y_test, \n",
    "                   y_pred, \n",
    "                   record=f, \n",
    "                   eval_model=eval_on)\n",
    "\n",
    "    \n",
    "    ''' Generating output file with probability score '''\n",
    "    predict_proba(predictions, \n",
    "                  X_test, \n",
    "                  y_test, \n",
    "                  y_pred, \n",
    "                  proba_file=proba_file, \n",
    "                  proba_on=proba_on)\n",
    "    \n",
    "    if proba_on:\n",
    "        print(\"\\nOutput file:'\" + proba_file + \"' Created\")\n",
    "        print(\"\\nOutput file:'\" + proba_file + \"' Created\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_train, \n",
    "         y_train, \n",
    "         X_val, \n",
    "         y_val,\n",
    "         X_test, \n",
    "         y_test,\n",
    "         mode,\n",
    "         datasize_change,\n",
    "         sample_balance,\n",
    "         balance_sampling_on,                                   \n",
    "         balance_sampling_type,\n",
    "         sample_ratio,\n",
    "         ratio,\n",
    "         sample_on, \n",
    "         sample_type,\n",
    "         tokenizer_file,\n",
    "         max_len,\n",
    "         batch_size,\n",
    "         epochs,\n",
    "         model_file,\n",
    "         eval_on, \n",
    "         proba_file,\n",
    "         proba_on,\n",
    "         result_file):\n",
    "    \n",
    "    ''' Open result file for records '''\n",
    "    record = open(result_file, \"a\")\n",
    "    \n",
    "    \n",
    "    ''' Check the processing time '''\n",
    "    proc_start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "    ''' Select a mode for training or testing'''\n",
    "    if mode == \"train\":\n",
    "        \n",
    "        model_train(X_train, \n",
    "                    y_train, \n",
    "                    X_val, \n",
    "                    y_val, \n",
    "                    datasize_change, \n",
    "                    sample_balance, \n",
    "                    balance_sampling_on, \n",
    "                    balance_sampling_type, \n",
    "                    sample_ratio, \n",
    "                    ratio, \n",
    "                    sample_on, \n",
    "                    sample_type, \n",
    "                    tokenizer_file, max_len, \n",
    "                    batch_size, \n",
    "                    epochs, \n",
    "                    model_file, \n",
    "                    result_file=record)\n",
    "    \n",
    "    elif mode == \"test\":\n",
    "        \n",
    "        model_inference(X_test, \n",
    "                        y_test, \n",
    "                        tokenizer_file, \n",
    "                        model_file, \n",
    "                        max_len, \n",
    "                        eval_on, \n",
    "                        proba_file, \n",
    "                        proba_on, \n",
    "                        result_file=record)\n",
    "    \n",
    "    \n",
    "    ''' Check the processing time '''\n",
    "    proc_elapsed = timeit.default_timer() - proc_start_time\n",
    "    \n",
    "    print(\"\\n************** Processing Time **************\")\n",
    "    print(\"\\n{}: {} sec\\n\".format(mode, round(proc_elapsed,2)))\n",
    "    print(\"\\n************** Processing Time **************\", file=record)\n",
    "    print(\"\\n{}: {} sec\\n\".format(mode, round(proc_elapsed,2)), file=record)\n",
    "    \n",
    "    print(\"\\nSummary file:'\" + result_file + \"' Created\")\n",
    "    print(\"\\nSummary file:'\" + result_file + \"' Created\", file=record)\n",
    "    \n",
    "    \n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5bxaV-8V7px"
   },
   "source": [
    "# 3. Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \n",
    "    ###############################################\n",
    "    ##########  1. Set Parameter Values  ##########\n",
    "    ###############################################\n",
    "\n",
    "    ########  1-1. Input file name  ########\n",
    "    input_filename=\"LER_rawdata.csv\" \n",
    "    \n",
    "    \n",
    "    ########  1-2. Which mode to run?  ########\n",
    "    mode_name = \"data-split\"                                    # 3 options: \"data-split\", \"train\", \"test\"\n",
    "                                                                # Use 'data-split' before 'train' or/and 'test'\n",
    "    \n",
    "    \n",
    "    ########  1-3. Data size change?  ########\n",
    "    ## 1-3-1. Change on/off?\n",
    "    datachange_on = 0                                           # 0 for no change; 1 for change of data size\n",
    "    \n",
    "    ## 1-3-2. class balance (1:1)?\n",
    "    balance_on = 0                                              # 0 for no balance; 1 for class balance (1:1)\n",
    "    balance_sample_on = 0                                       # 0 for no sampling; 1 for sampling\n",
    "    balance_sample_type = 'under'                               # 'over'(oversampling); 'under'(undersampling)\n",
    "    balance_str = 'balance' + str(balance_on) + '_'\n",
    "    \n",
    "    ## 1-3-3. data increase?\n",
    "    ratio_on = 0                                                # 0 for no ratio; 1 for ratio for data size\n",
    "    ratio_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # a list containing ratio numbers\n",
    "\n",
    "    \n",
    "    ########  1-4. Sampling applied?  ########\n",
    "    sampling_on = 0                                             # 0 for no sampling; 1 for sampling\n",
    "    sampling_type = 'over'                                      # 'over'(oversampling)/'under'(undersampling)\n",
    "    \n",
    "    \n",
    "    ########  1-5. Check token distribution for deciding a MAX_LEN value: uncommentize if needed\n",
    "    #print(\"\\n************** Token Distribution **************\")\n",
    "    #X, y = load_data(input_filename, record=None)\n",
    "    #token_distribution(X)\n",
    "\n",
    "    \n",
    "    ########  1-6. Hyperparameters for CNN model  ########\n",
    "    MAX_LEN = 150                                               \n",
    "    BATCH_SIZE = 16                                             \n",
    "    EPOCHS = 4                                                \n",
    "\n",
    "   \n",
    "    ########  1-7. Evaluation & probability file  ######## \n",
    "    eval_on = 1                                                 # 0 for no; 1 for yes (evaluation scores)\n",
    "    proba_on = 1                                                # 0 for no; 1 for yes (probability & prediction output)\n",
    "    \n",
    "    \n",
    "\n",
    "    ###############################################\n",
    "    ##########   2. Run Main Fuction    ###########\n",
    "    ###############################################\n",
    "    \n",
    "    if mode_name == \"data-split\":\n",
    "        \n",
    "        result_file = \"summary_cnn_\" + mode_name + \".txt\" \n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = split_data(input_filename, result_file)\n",
    "        \n",
    "    else:\n",
    "        if datachange_on:                   \n",
    "            for ratio in ratio_list:           \n",
    "                if sampling_on:\n",
    "                    tokenizer_file=\"tokenizer_cnn_\"+balance_str+str(ratio)+\"_\"+sampling_type+\".pickle\"\n",
    "                    model_file=\"model_cnn_\"+balance_str+str(ratio)+\"_\"+sampling_type+\".keras\"\n",
    "                    proba_file=\"result_cnn_\"+balance_str+str(ratio)+\"_\"+sampling_type+\".csv\"  \n",
    "                    eval_file=\"summary_cnn_\"+mode_name+\"_\"+balance_str+str(ratio)+\"_\"+sampling_type+\".txt\" \n",
    "                else:\n",
    "                    tokenizer_file=\"tokenizer_cnn_\"+balance_str+str(ratio)+\".pickle\"\n",
    "                    model_file=\"model_cnn_\"+balance_str+str(ratio)+\".keras\"\n",
    "                    proba_file=\"result_cnn_\"+balance_str+str(ratio)+\".csv\"  \n",
    "                    eval_file=\"summary_cnn_\"+mode_name+\"_\"+balance_str+str(ratio)+\".txt\"  \n",
    "            \n",
    "                main(X_train, \n",
    "                     y_train, \n",
    "                     X_val, \n",
    "                     y_val, \n",
    "                     X_test, \n",
    "                     y_test, \n",
    "                     mode=mode_name,\n",
    "                     datasize_change=datachange_on, \n",
    "                     sample_balance=balance_on,\n",
    "                     balance_sampling_on=balance_sample_on,\n",
    "                     balance_sampling_type=balance_sample_type,\n",
    "                     sample_ratio=ratio_on, \n",
    "                     ratio=ratio, \n",
    "                     sample_on=sampling_on, \n",
    "                     sample_type=sampling_type, \n",
    "                     tokenizer_file=tokenizer_file,\n",
    "                     max_len=MAX_LEN, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=EPOCHS,\n",
    "                     model_file=model_file, \n",
    "                     eval_on=eval_on, \n",
    "                     proba_file=proba_file,\n",
    "                     proba_on=proba_on, \n",
    "                     result_file=eval_file)\n",
    "        else:\n",
    "            if sampling_on:\n",
    "                tokenizer_file = \"tokenizer_cnn_\"+sampling_type+\".pickle\"\n",
    "                model_file = \"model_cnn_\"+sampling_type+\".keras\"\n",
    "                proba_file = \"result_cnn_\"+sampling_type+\".csv\"  \n",
    "                eval_file = \"summary_cnn_\"+mode_name+\"_\"+sampling_type+\".txt\" \n",
    "            else:\n",
    "                tokenizer_file = \"tokenizer_cnn.pickle\"\n",
    "                model_file = \"model_cnn.keras\"\n",
    "                proba_file = \"result_cnn.csv\"  \n",
    "                eval_file = \"summary_cnn_\"+mode_name+\".txt\" \n",
    "            \n",
    "            main(X_train, \n",
    "                 y_train, \n",
    "                 X_val, \n",
    "                 y_val, \n",
    "                 X_test, \n",
    "                 y_test, \n",
    "                 mode=mode_name,\n",
    "                 datasize_change=datachange_on, \n",
    "                 sample_balance=balance_on,\n",
    "                 balance_sampling_on=balance_sample_on,\n",
    "                 balance_sampling_type=balance_sample_type,\n",
    "                 sample_ratio=ratio_on, \n",
    "                 ratio=1, \n",
    "                 sample_on=sampling_on, \n",
    "                 sample_type=sampling_type, \n",
    "                 tokenizer_file=tokenizer_file,\n",
    "                 max_len=MAX_LEN, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 epochs=EPOCHS,\n",
    "                 model_file=model_file, \n",
    "                 eval_on=eval_on, \n",
    "                 proba_file=proba_file,\n",
    "                 proba_on=proba_on, \n",
    "                 result_file=eval_file)\n",
    "\n",
    "            \n",
    "    print(\"\\n\\n************** Processing Completed **************\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
