{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SoFLIx-rF-N"
   },
   "source": [
    "# LERCause: Causal Sentence Identification with LER (Nuclear Safety Reports)\n",
    "\n",
    "This code is to run traditional machine learning models on LER data for sentence classification and prediction. \n",
    "\n",
    "Author:   \n",
    "1. Jinmo Kim: School of Information Sciences, University of Illinois Urbana-Champaign   \n",
    "2. Jenna Kim: School of Information Sciences, Univeristy of Illinois Urbana-Champaign      \n",
    "\n",
    "Cite this paper:   \n",
    "\n",
    "Kim, J., Kim, J., Lee, A., Kim, J., Diesner, J. (2024). LERCause: Deep learning approaches for causal sentence identification from nuclear safety reports. Plos One.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfkWZ8RTJkSa"
   },
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCQmTfQRrF-S"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OIb10PFrF-U",
    "outputId": "c353157b-d061-48ab-d07a-34b6e8299266"
   },
   "outputs": [],
   "source": [
    "## Install imbalanced-learn library for sampling if not already installed\n",
    " \n",
    "#!pip install imbalanced-learn==0.11.0\n",
    "\n",
    "#!pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3m2VPlSJomr"
   },
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWnA_g5SrF-V"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, record):   \n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file   \n",
    "    record: text file to include a processing output\n",
    "    \n",
    "    return two dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Read in data from input file '''\n",
    "    df = pd.read_csv(filename, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    ''' Display no of rows and columns '''\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record)\n",
    "    \n",
    "    \n",
    "    ''' Select data needed for processing & rename columns '''\n",
    "    df = df[['PMID', 'USENID', 'SENT', 'CLASS']]\n",
    "    df.rename({\"PMID\": \"pmid\", \"USENID\": \"usenid\", \"SENT\": \"sentence\", \"CLASS\": \"label\"}, \n",
    "              axis=1, \n",
    "              inplace=True)\n",
    "    \n",
    "    \n",
    "    ''' Check the first few instances '''  \n",
    "    print(\"\\n<Data View: First Few Instances>\")\n",
    "    print(\"\\n\", df.head()) \n",
    "    print(\"\\n<Data View: First Few Instances>\", file=record)\n",
    "    print(\"\\n\", df.head(), file=record)\n",
    "    \n",
    "    \n",
    "    ''' Display no of lables and rows ''' \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)\n",
    "    \n",
    "    \n",
    "    ''' Split data into X and y '''\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    \n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6UoMg2jrF-W"
   },
   "outputs": [],
   "source": [
    "def sample_data(X_train, \n",
    "                y_train, \n",
    "                record, \n",
    "                sampling=0, \n",
    "                sample_method='over'):  \n",
    "    \"\"\"\n",
    "       Sampling input train data\n",
    "       \n",
    "       X_train: dataframe of X train data\n",
    "       y_train: datafram of y train data\n",
    "       record: text file including a processing output\n",
    "       sampling: indicator of sampling funtion is on or off\n",
    "       sample_method: method of sampling (oversampling or undersampling)\n",
    "       \n",
    "       return two sampled dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    \n",
    "    \n",
    "    ''' Select a sampling method '''\n",
    "    if sampling:\n",
    "        if sample_method == 'over':\n",
    "            oversample = RandomOverSampler(random_state=42)\n",
    "            X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "            print('\\n************** Data Sampling **************')\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()))\n",
    "            print('\\n************** Data Sampling **************', file=record)\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()), file=record)\n",
    "            \n",
    "            X_train_sam, y_train_sam = X_over, y_over\n",
    "            \n",
    "        elif sample_method == 'under':\n",
    "            undersample = RandomUnderSampler(random_state=42)\n",
    "            X_under, y_under = undersample.fit_resample(X_train, y_train)\n",
    "        \n",
    "            print('\\n************** Data Sampling **************')\n",
    "            print('\\nUndersampled Data (class, Rows):\\n{}'.format(y_under.value_counts()))\n",
    "            print('\\n************** Data Sampling **************', file=record)\n",
    "            print('\\nUndersampled Data (class, Rows):\\n{}'.format(y_under.value_counts()), file=record)\n",
    "            \n",
    "            X_train_sam, y_train_sam = X_under, y_under\n",
    "    else:\n",
    "        X_train_sam, y_train_sam = X_train, y_train \n",
    "        \n",
    "        print('\\n************** Data Sampling **************')\n",
    "        print('\\nNo Sampling Performed\\n')\n",
    "        print('\\n************** Data Sampling **************', file=record)\n",
    "        print('\\nNo Sampling Performed\\n', file=record)\n",
    "    \n",
    "    return X_train_sam, y_train_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx6Q8w3lrF-X"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_data_raw):\n",
    "    \"\"\"\n",
    "       Preprocess data with lowercase conversion, punctuation removal, tokenization, stemming\n",
    "       \n",
    "       X_data_raw: X data in dataframe\n",
    "       \n",
    "       return dataframe  \n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Make sure that data type is a string '''\n",
    "    X_data=X_data_raw.iloc[:, -1].astype(str)\n",
    "   \n",
    "\n",
    "    ''' Convert all characters to lowercase '''\n",
    "    X_data = X_data.map(lambda x: x.lower())\n",
    "\n",
    "    \n",
    "    ''' Remove punctuation '''\n",
    "    X_data = X_data.str.replace('[^\\w\\s]', '')\n",
    "    \n",
    "    \n",
    "    ''' Tokenization '''\n",
    "    X_data = X_data.apply(nltk.word_tokenize)\n",
    "    \n",
    "    \n",
    "    ''' Remove stopwords '''\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    X_data = X_data.apply(lambda x: [word for word in x if word not in stopword_list])\n",
    "\n",
    "          \n",
    "    ''' Stemming '''\n",
    "    stemmer = PorterStemmer()\n",
    "    X_data = X_data.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "   \n",
    "    \n",
    "    ''' Integrate a list of elements into a string '''\n",
    "    X_data = X_data.apply(lambda x: ' '.join(x)) \n",
    "    \n",
    "    \n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Nnyw5kYrF-X"
   },
   "outputs": [],
   "source": [
    "def select_classifier(model):   \n",
    "    \"\"\"\n",
    "      Options of classifiers:\n",
    "      decision tree, svm, naive bayes, random forest, and gradient boosting\n",
    "      \n",
    "      model: name of classifier algorithm\n",
    "      \n",
    "      return a selected classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    if model=='DT':\n",
    "        classifier = DecisionTreeClassifier(max_depth=2)\n",
    "        \n",
    "    elif model=='SVM':\n",
    "        classifier = SVC(kernel='linear', probability=True)  \n",
    "    \n",
    "    elif model=='NB':\n",
    "        classifier = MultinomialNB()\n",
    "    \n",
    "    elif model=='LR':\n",
    "        classifier = LogisticRegression()  \n",
    "    \n",
    "    elif model=='RF':\n",
    "        classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    \n",
    "    elif model=='GB':\n",
    "        classifier = GradientBoostingClassifier()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Must be a valid classifier name!\")\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctZ80-2prF-Y"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, \n",
    "                   y_pred, \n",
    "                   record, \n",
    "                   eval_model=0):\n",
    "    \"\"\"\n",
    "      evaluate a model performance\n",
    "      \n",
    "      y_test: original y test data\n",
    "      y_pred: predicted y values\n",
    "      record: text file containing a processing output\n",
    "      eval_model: indicator if this funtion is on or off\n",
    "    \"\"\"\n",
    "    \n",
    "    if eval_model:\n",
    "        \n",
    "        ''' Create a confusion matrix '''\n",
    "        print('\\nConfusion Matrix:\\n')\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print('\\nConfusion Matrix:\\n', file=record)\n",
    "        print(confusion_matrix(y_test, y_pred), file=record)\n",
    "        \n",
    "        ''' Display a classification report '''\n",
    "        print('\\nClassification Report:\\n')\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "        print('\\nClassification Report:\\n', file=record)\n",
    "        print(classification_report(y_test, y_pred, digits=4), file=record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8ikM_EPrF-Z"
   },
   "outputs": [],
   "source": [
    "def predict_proba(model, \n",
    "                  X_test_proc, \n",
    "                  X_test, \n",
    "                  y_test, \n",
    "                  y_pred, \n",
    "                  proba_file, \n",
    "                  proba_on=0):\n",
    "    \"\"\"\n",
    "       Predict probability of each class\n",
    "       \n",
    "       model: trained model with a selected classifier\n",
    "       X_test_proc: X test data preprocessed\n",
    "       X_test: original X test data\n",
    "       y_test: original y test data\n",
    "       y_pred: predicted y values\n",
    "       proba_file: output file containing probability scores\n",
    "       proba_on: indicator if the probability output is expected    \n",
    "    \"\"\"\n",
    "    \n",
    "    if proba_on:\n",
    "        \n",
    "        ''' Compute probability '''\n",
    "        y_prob = model.predict_proba(X_test_proc)\n",
    "        df_prob = pd.DataFrame(data=y_prob, columns=model.classes_)\n",
    "        result = pd.concat([X_test.reset_index(drop=True), df_prob], \n",
    "                           axis=1, \n",
    "                           ignore_index=False)\n",
    "    \n",
    "    \n",
    "        ''' Add predicted class to output '''\n",
    "        result['pred'] = pd.Series(y_pred)\n",
    "\n",
    "        ''' Add actual class to output ''' \n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        result['act'] = y_test\n",
    "\n",
    "        ''' Save output '''\n",
    "        result.to_csv(proba_file, \n",
    "                      encoding='utf-8', \n",
    "                      index=False, \n",
    "                      header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(input_file, \n",
    "               result_file):\n",
    "    \"\"\"\n",
    "       Split data from input file\n",
    "       \n",
    "       input_file: file containing input data \n",
    "       result_file: name of output file of evaluation\n",
    "       \n",
    "       return X and y dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Open result file for records '''\n",
    "    f=open(result_file, \"a\")\n",
    "    \n",
    "    \n",
    "    ''' Load data '''\n",
    "    print(\"\\n************** Loading Data ************\\n\")\n",
    "    print(\"\\n************** Loading Data ************\\n\", file=f)\n",
    "    \n",
    "    X, y = load_data(input_file, record=f)\n",
    "    \n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]))\n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]), file=f)\n",
    "    \n",
    "    \n",
    "    ''' Train and test split '''\n",
    "    print(\"\\n************** Spliting Data **************\\n\")\n",
    "    print(\"\\n************** Spliting Data **************\\n\", file=f)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test,y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(X_train.shape))\n",
    "    print(\"Val Data: {}\".format(X_val.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "    print(\"Train Data: {}\".format(X_train.shape), file=f)\n",
    "    print(\"Val Data: {}\".format(X_val.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(X_test.shape), file=f)\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train')\n",
    "    print(y_train.value_counts())\n",
    "    print('\\nClass Counts(label, row): Val')\n",
    "    print(y_val.value_counts())\n",
    "    print('\\nClass Counts(label, row): Test')\n",
    "    print(y_test.value_counts())\n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(y_train.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Val', file=f)\n",
    "    print(y_val.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(y_test.value_counts(), file=f)\n",
    "    \n",
    "    print(\"\\n<X_test Data>\")\n",
    "    print(X_test.head())\n",
    "    print(\"\\n<X_test Data>\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "        \n",
    "        \n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X_train, \n",
    "                y_train,\n",
    "                datasize_change,\n",
    "                sample_balance,\n",
    "                balance_sampling_on,                                   \n",
    "                balance_sampling_type,\n",
    "                sample_ratio,\n",
    "                ratio,\n",
    "                sample_on, \n",
    "                sample_type, \n",
    "                model_method,\n",
    "                model_file,\n",
    "                result_file):   \n",
    "    \"\"\"\n",
    "       Function for data processing and model fitting\n",
    "       \n",
    "       X_train: dataframe containing X train data \n",
    "       y_train: dataframe containing y train data\n",
    "       datasize_change: data size change on or off\n",
    "       sample_balance: balance of sample on or off\n",
    "       balance_sampling_on: sampling on or off when balance is 1\n",
    "       balance_samplling_type: sample type to choose if balance_sampling_on is 1\n",
    "       sample_ratio: proportion of data size for balance sampling\n",
    "       ratio: proportion of data size\n",
    "       sample_on: sampling on or off\n",
    "       sample_type: sample type to choose if sample_on is 1\n",
    "       model_method: name of classifier to be applied for model fitting\n",
    "       model_file: file saved trained model\n",
    "       result_file: name of output file of evaluation  \n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Open result file for records '''\n",
    "    f = result_file\n",
    "    \n",
    "    \n",
    "    ''' Data size change '''\n",
    "    if datasize_change:\n",
    "        \n",
    "        ''' Sample data with balance (1:1) '''\n",
    "        if sample_balance:\n",
    "            \n",
    "            print(\"\\n************** Data Balancing: Label Class (1:1) *************\\n\")\n",
    "            print(\"\\n************** Data Balancing: Label Class (1:1) *************\\n\", file=f)\n",
    "            \n",
    "            X_train, y_train = sample_data(X_train, \n",
    "                                           y_train, \n",
    "                                           record=f, \n",
    "                                           sampling=balance_sampling_on, \n",
    "                                           sample_method=balance_sampling_type)\n",
    "            \n",
    "            print('\\nClass Counts(label, row): After balancing')\n",
    "            print(y_train.value_counts())\n",
    "            print('\\nClass Counts(label, row): After balancing', file=f)\n",
    "            print(y_train.value_counts(), file=f)\n",
    "            \n",
    "            print(\"\\n<Balanced Train Data>\")\n",
    "            print(X_train.head()) \n",
    "            print(\"\\n<Balanced Train Data>\", file=f)\n",
    "            print(X_train.head(), file=f)\n",
    "        \n",
    "                  \n",
    "        ''' Sample data based on size ratio '''    \n",
    "        if sample_ratio:\n",
    "            \n",
    "            if ratio == 1:\n",
    "                X_train = X_train\n",
    "                y_train = y_train       \n",
    "            \n",
    "            else:\n",
    "                X_train, _, y_train, _ = train_test_split(X_train, \n",
    "                                                          y_train, \n",
    "                                                          train_size=ratio, \n",
    "                                                          random_state=42, \n",
    "                                                          stratify=y_train)\n",
    "            \n",
    "            print(\"\\n************** Data Size Change: Ratio *************\\n\")\n",
    "            print(\"Data Ratio: {}\".format(ratio))   \n",
    "            print(\"\\n************** Data Size Change: Ratio *************\\n\", file=f)\n",
    "            print(\"Data Ratio: {}\".format(ratio), file=f)\n",
    "            \n",
    "            print('\\nClass Counts(label, row): After sampling')\n",
    "            print(y_train.value_counts())\n",
    "            print('\\nClass Counts(label, row): After sampling', file=f)\n",
    "            print(y_train.value_counts(), file=f)\n",
    "            \n",
    "            print(\"\\n<Train Data Based on Ratio>\")\n",
    "            print(X_train.head())\n",
    "            print(\"\\n<Train Data Based on Ratio>\", file=f)\n",
    "            print(X_train.head(), file=f)\n",
    "            \n",
    "        \n",
    "        ''' Reset index '''\n",
    "        X_train=X_train.reset_index(drop=True)\n",
    "        y_train=y_train.reset_index(drop=True)\n",
    "        \n",
    "        print(\"\\n************** Processing Data **************\")\n",
    "        print(\"\\nTrain Data: {}\".format(X_train.shape))\n",
    "        print(\"\\n************** Processing Data **************\", file=f)\n",
    "        print(\"\\nTrain Data: {}\".format(X_train.shape), file=f)\n",
    "        \n",
    "        print('\\nClass Counts(label, row): Train')\n",
    "        print(y_train.value_counts())\n",
    "        print('\\nClass Counts(label, row): Train', file=f)\n",
    "        print(y_train.value_counts(), file=f)\n",
    "        \n",
    "        print(\"\\n<X_train Data>\")\n",
    "        print(X_train.head())\n",
    "        print(\"\\n<X_train Data>\", file=f)\n",
    "        print(X_train.head(), file=f)\n",
    "\n",
    "   \n",
    "    ''' Sampling '''\n",
    "    if sample_on:\n",
    "        \n",
    "        X_train, y_train = sample_data(X_train, \n",
    "                                       y_train, \n",
    "                                       record=f, \n",
    "                                       sampling=sample_on, \n",
    "                                       sample_method=sample_type)\n",
    "        \n",
    "        print(\"\\nSampled Data: First Few Instances\")\n",
    "        print(X_train.head(3))\n",
    "        print(\"\\nSampled Data: First Few Instances\", file=f)\n",
    "        print(X_train.head(3), file=f)\n",
    "        \n",
    "\n",
    "    ''' Preprocessing ''' \n",
    "    X_train_proc = preprocess_data(X_train)\n",
    "    \n",
    "    print(\"\\n<After preprocessing training data>\\n\")\n",
    "    print(X_train_proc)\n",
    "    print(\"\\n<After preprocessing training data>\\n\", file=f)\n",
    "    print(X_train_proc, file=f)\n",
    "    \n",
    "    \n",
    "    ''' Fitting a model '''\n",
    "    print(\"\\n************** Training Model: \" + model_method + \" **************\")\n",
    "    print(\"\\n************** Training Model: \" + model_method + \" **************\", file=f)\n",
    "    \n",
    "    \n",
    "    ''' Instantiate a transformer and classifier '''\n",
    "    transformer = TfidfVectorizer(use_idf=True)\n",
    "    classifier = select_classifier(model_method)\n",
    "    \n",
    "    \n",
    "    ''' Define pipeline to combine transformer and classifier '''\n",
    "    pipe = Pipeline([(\"transformer\", transformer),(\"classifier\", classifier)])\n",
    "\n",
    "    \n",
    "    ''' Fit the pipeline on entire training data '''\n",
    "    estimator = pipe.fit(X_train_proc, y_train)\n",
    "    \n",
    "    print(\"\\nA model by '\" + model_method + \"' created\\n\")\n",
    "    print(\"\\nA model by '\" + model_method + \"' created\\n\", file=f)\n",
    "              \n",
    "        \n",
    "    ''' Saving fitted model '''\n",
    "    pickle.dump(estimator, open(model_file, 'wb'))\n",
    "    \n",
    "    print(\"Trained model saved in the local directory\\n\")\n",
    "    print(\"Trained model saved in the local directory\\n\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(X_test,\n",
    "                    y_test,  \n",
    "                    model_method,\n",
    "                    model_file,\n",
    "                    eval_on, \n",
    "                    proba_file,\n",
    "                    proba_on,\n",
    "                    result_file):   \n",
    "    \"\"\"\n",
    "       Function for prediction and evaluation\n",
    "       \n",
    "       X_test: dataframe containing X test data \n",
    "       y_test: dataframe containing y test data\n",
    "       model_method: name of classifier to be applied for model fitting\n",
    "       model_file: file saved trained model\n",
    "       eval_on: model evaluation on or off\n",
    "       proba_file: name of output file of probability\n",
    "       proba_on: probability on or off\n",
    "       result_file: name of output file of evaluation \n",
    "    \"\"\"\n",
    "      \n",
    "    ''' Open result file for records '''\n",
    "    f = result_file\n",
    "    \n",
    "    ''' Load trained model ''' \n",
    "    model = pickle.load(open(model_file, 'rb'))\n",
    "    \n",
    "    print(\"\\nA trained model from '\" + model_file + \"' loaded\")\n",
    "    print(\"\\nA trained model from '\" + model_file + \"' loaded\", file=f)\n",
    "                    \n",
    "\n",
    "    ''' Predict output '''\n",
    "    print(\"\\n************** Getting Predictions **************\")\n",
    "    print(\"\\n************** Getting Predictions **************\", file=f)\n",
    "    \n",
    "    X_test_proc = preprocess_data(X_test)\n",
    "    \n",
    "    print(\"\\n<After preprocessing test data>\")\n",
    "    print(X_test_proc)\n",
    "    print(\"\\n<After preprocessing test data>\", file=f)\n",
    "    print(X_test_proc, file=f)\n",
    "    \n",
    "    y_pred = model.predict(X_test_proc)\n",
    "    \n",
    "\n",
    "    ''' Evaluate model performance '''\n",
    "    print(\"\\n************** Evaluating Performance **************\")\n",
    "    print(\"\\n************** Evaluating Performance **************\", file=f)\n",
    "    \n",
    "    evaluate_model(y_test, \n",
    "                   y_pred, \n",
    "                   record=f, \n",
    "                   eval_model=eval_on)\n",
    "    \n",
    "\n",
    "    ''' Generate output with probability scores '''\n",
    "    predict_proba(model, \n",
    "                  X_test_proc, \n",
    "                  X_test, \n",
    "                  y_test, \n",
    "                  y_pred, \n",
    "                  proba_file=proba_file, \n",
    "                  proba_on=proba_on)\n",
    "    \n",
    "    if proba_on:\n",
    "        print(\"\\nOutput file:'\" + proba_file + \"' Created\")\n",
    "        print(\"\\nOutput file:'\" + proba_file + \"' Created\", file=f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_train, \n",
    "         y_train, \n",
    "         X_test, \n",
    "         y_test,\n",
    "         mode,\n",
    "         datasize_change,\n",
    "         sample_balance,\n",
    "         balance_sampling_on,                                   \n",
    "         balance_sampling_type,\n",
    "         sample_ratio,\n",
    "         ratio,\n",
    "         sample_on, \n",
    "         sample_type, \n",
    "         model_method,\n",
    "         model_file,\n",
    "         eval_on, \n",
    "         proba_file,\n",
    "         proba_on,\n",
    "         result_file):\n",
    "    \n",
    "    ''' Open result file for records '''\n",
    "    record = open(result_file, \"a\")\n",
    "    \n",
    "    \n",
    "    ''' Check the processing time '''\n",
    "    proc_start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "    ''' Select a mode for training or testing'''\n",
    "    if mode == \"train\":\n",
    "        \n",
    "        model_train(X_train, \n",
    "                    y_train, \n",
    "                    datasize_change, \n",
    "                    sample_balance, \n",
    "                    balance_sampling_on, \n",
    "                    balance_sampling_type, \n",
    "                    sample_ratio, \n",
    "                    ratio, \n",
    "                    sample_on, \n",
    "                    sample_type, \n",
    "                    model_method, \n",
    "                    model_file, \n",
    "                    result_file=record)\n",
    "    \n",
    "    elif mode == \"test\":\n",
    "        \n",
    "        model_inference(X_test, \n",
    "                        y_test, \n",
    "                        model_method, \n",
    "                        model_file, \n",
    "                        eval_on, \n",
    "                        proba_file, \n",
    "                        proba_on, \n",
    "                        result_file=record)\n",
    "    \n",
    "    \n",
    "    ''' Check the processing time '''\n",
    "    proc_elapsed = timeit.default_timer() - proc_start_time\n",
    "    \n",
    "    print(\"\\n************** Processing Time **************\")\n",
    "    print(\"\\n{}: {} sec\\n\".format(mode, round(proc_elapsed,2)))\n",
    "    print(\"\\n************** Processing Time **************\", file=record)\n",
    "    print(\"\\n{}: {} sec\\n\".format(mode, round(proc_elapsed,2)), file=record)\n",
    "    \n",
    "    print(\"\\nSummary file:'\" + result_file + \"' Created\")\n",
    "    print(\"\\nSummary file:'\" + result_file + \"' Created\", file=record)\n",
    "    \n",
    "    \n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TaHAxbjJb_-"
   },
   "source": [
    "# 3. Run Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYt_qbIcrF-a",
    "outputId": "fe7b10ae-d255-4b6b-cae9-d4af8d098c65",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \n",
    "    ###############################################\n",
    "    ##########  1. Set Parameter Values  ##########\n",
    "    ###############################################\n",
    "\n",
    "    ########  1-1. Input file name  ########\n",
    "    input_filename=\"LER_rawdata.csv\" \n",
    "    \n",
    "    \n",
    "    ########  1-2. Which mode to run?  ########\n",
    "    mode_name = \"data-split\"                                    # 3 options: \"data-split\", \"train\", \"test\"\n",
    "                                                                # Use \"data-split\" before \"train\" or/and \"test\"\n",
    "\n",
    "    ########  1-3. Data size change?  ########\n",
    "    ## 1-3-1. Change on/off?\n",
    "    datachange_on = 0                                           # 0 for no change; 1 for change of data size\n",
    "    \n",
    "    ## 1-3-2. class balance (1:1)?\n",
    "    balance_on = 0                                              # 0 for no balance; 1 for class balance (1:1)\n",
    "    balance_sample_on = 0                                       # 0 for no sampling; 1 for sampling\n",
    "    balance_sample_type = 'under'                               # 'over'(oversampling); 'under'(undersampling)\n",
    "    balance_str = 'balance' + str(balance_on) + '_'\n",
    "    \n",
    "    ## 1-3-3. data increase?\n",
    "    ratio_on = 0                                                # 0 for no ratio; 1 for ratio for data size\n",
    "    ratio_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # a list containing ratio numbers\n",
    "\n",
    "    \n",
    "    ########  1-4. Sampling applied?  ########\n",
    "    sampling_on = 0                                             # 0 for no sampling; 1 for sampling\n",
    "    sampling_type = 'over'                                      # 'over'(oversampling)/'under'(undersampling)\n",
    "    \n",
    "    \n",
    "    ########  1-5. Which model to use?  ########\n",
    "    model_type = 'LR'                                           # 'LR'(Logisitic Regression);\n",
    "                                                                # 'SVM'(Support Vector Machine);'NB'(Naive Bayes);\n",
    "                                                                # 'RF'(Random Forest);'GB'(Gradient Boosting)\n",
    "    \n",
    "    ########  1-6. Evaluation & probability file  ######## \n",
    "    eval_on = 1                                                 # 0 for no; 1 for yes (evaluatin scores)\n",
    "    proba_on = 1                                                # 0 for no; 1 for yes (probability & prediction output)\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    ##########   2. Run Main Fuction    ###########\n",
    "    ###############################################\n",
    "    \n",
    "    if mode_name == \"data-split\":\n",
    "        result_file = \"summary_ml_\" + mode_name + \".txt\" \n",
    "        X_train, y_train, X_test, y_test = split_data(input_filename, result_file)\n",
    "        \n",
    "    else:\n",
    "        if datachange_on:                   \n",
    "            for ratio in ratio_list:           \n",
    "                if sampling_on:\n",
    "                    model_file=\"model_ml_\"+balance_str+str(ratio)+\"_\"+sampling_type+\"_\"+model_type+\".sav\"\n",
    "                    proba_file=\"result_ml_\"+balance_str+str(ratio)+\"_\"+sampling_type+\"_\"+model_type+\".csv\"  \n",
    "                    eval_file=\"summary_ml_\"+mode_name+balance_str+\"_\"+str(ratio)+\"_\"+sampling_type+\"_\"+model_type+\".txt\" \n",
    "                else:\n",
    "                    model_file=\"model_ml_\"+balance_str+str(ratio)+\"_\"+model_type+\".sav\"\n",
    "                    proba_file=\"result_ml_\"+balance_str+str(ratio)+\"_\"+model_type+\".csv\"  \n",
    "                    eval_file=\"summary_ml_\"+mode_name+balance_str+str(ratio)+\"_\"+model_type+\".txt\"  \n",
    "            \n",
    "                main(X_train, y_train, X_test, y_test, mode=mode_name,\n",
    "                     datasize_change=datachange_on, sample_balance=balance_on,\n",
    "                     balance_sampling_on=balance_sample_on,\n",
    "                     balance_sampling_type=balance_sample_type,\n",
    "                     sample_ratio=ratio_on, ratio=ratio, sample_on=sampling_on, \n",
    "                     sample_type=sampling_type, model_method=model_type, \n",
    "                     model_file=model_file, eval_on=eval_on, proba_file=proba_file,\n",
    "                     proba_on=proba_on, result_file=eval_file)\n",
    "        else:\n",
    "            if sampling_on:\n",
    "                model_file=\"model_ml_\"+sampling_type+\"_\"+model_type+\".sav\"\n",
    "                proba_file=\"result_ml_\"+sampling_type+\"_\"+model_type+\".csv\"  \n",
    "                eval_file=\"summary_ml_\"+mode_name+\"_\"+sampling_type+\"_\"+model_type+\".txt\" \n",
    "            else:\n",
    "                model_file=\"model_ml_\"+model_type+\".sav\"\n",
    "                proba_file=\"result_ml_\"+model_type+\".csv\"  \n",
    "                eval_file=\"summary_ml_\"+mode_name+\"_\"+model_type+\".txt\" \n",
    "            \n",
    "            main(X_train, y_train, X_test, y_test, mode=mode_name,\n",
    "                 datasize_change=datachange_on, sample_balance=balance_on,\n",
    "                 balance_sampling_on=balance_sample_on,\n",
    "                 balance_sampling_type=balance_sample_type,\n",
    "                 sample_ratio=ratio_on, ratio=1, sample_on=sampling_on, \n",
    "                 sample_type=sampling_type, model_method=model_type, \n",
    "                 model_file=model_file, eval_on=eval_on, proba_file=proba_file,\n",
    "                 proba_on=proba_on, result_file=eval_file)\n",
    "\n",
    "                \n",
    "    print(\"\\n\\n************** Processing Completed **************\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
